near term gp experiments (nov 23):
-----
- dense with gemini full, 48
- gp tuned with gemini, 48, full ... does dim red matter
- try no observation noise

- look deeper... 300?, 1000?
- test gemini at higher embedding counts (if that's our embedder, easy to test effect of dimensionality)

-try refitting
- try gradient enchanced
-ucb

-warm start? 25%,50%,75% ... some warm start seems like a good idea, the start of the ranking should be quite informative ... can use gp full embeddings

- contextual bandits

- future... adaptive gp? once there is no relevant doc in a batch (for x batches) switch modes to gp? will this be hard to explain?

- for GP query decomposition, could asign each new query a relevance label (e.g. 2, 1.5) to seed the new area

Larger observation counts (e.g. 1000s)
- for 1000 observations, why so long (e.g. 10 mins per query... ) acquistion times don't seem to be going up too much for TS (dominated by the sampling?),
 but greedy increasse from 0.02s to 1s per iteration at begining vs end (of 1000 obs) -- may need to switch to inducing points?

 Speedup considerations (see chatgpt convo for brainstorming "Slow greedy sampling fix"):
- Fantasy models (gpytorch for bayesian updates)
- fast_prediction approximations in gpytorch
- sparse gps 